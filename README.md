# ğŸš€ Scalable Data Pipeline using dbt + Snowflake + S3

This project showcases a **production-grade cloud data pipeline** that ingests, transforms, and analyzes over **20 million records** using modern data engineering tools.

---

## ğŸ“Š Architecture Overview

![Architecture Diagram](/123.drawio.png)

---

## ğŸ”§ Project Description

The pipeline follows the ELT paradigm:

- **Extract & Load**:
  - Load raw Netflix dataset (20M+ records) into **Amazon S3**
  - Ingest S3 data into **Snowflake** data warehouse (raw layer)

- **Transform**:
  - Use **dbt (Data Build Tool)** to create structured models:
    - Layered modeling: `raw â†’ staging â†’ development`
    - Incremental materializations
    - Seeds for static data
    - Modular, version-controlled SQL logic

- **Test & Document**:
  - Built-in **schema tests**: `not_null`, `unique`, `relationships`
  - Auto-generated dbt **documentation & lineage graphs**

- **Visualize**:
  - Connect Snowflake models to **Power BI / Looker Studio / Tableau** for dashboards

---

## ğŸ“ Project Structure

```bash
netflix-DBT-Snowflake/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ raw/              # Source models from Snowflake tables
â”‚   â”œâ”€â”€ staging/          # Cleaned models with standardized formats
â”‚   â””â”€â”€ development/      # Final models with business logic
â”œâ”€â”€ seeds/                # Static lookup CSVs
â”œâ”€â”€ tests/                # Custom dbt tests
â”œâ”€â”€ macros/               # Reusable SQL + Jinja logic
â”œâ”€â”€ dbt_project.yml       # DBT project config
â”œâ”€â”€ packages.yml          # DBT dependency packages
â””â”€â”€ README.md             # Project documentation
